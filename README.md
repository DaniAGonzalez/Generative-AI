Generative AI and Deep Learning Architectures

This repository contains code, implementations, and projects developed while learning and applying advanced concepts in Generative Artificial Intelligence and Deep Learning. It reflects practical experience in designing, training, and analyzing modern neural network architectures and generative models.

The work includes fundamental topics such as neural network design, gradient descent, backpropagation, batch normalization, regularization, and dropout. It also covers the implementation of Convolutional Neural Networks and autoregressive models for feature extraction and sequence prediction, along with principles of maximum likelihood estimation and representation learning.

Recurrent Neural Networks, LSTMs, and bidirectional architectures were explored to handle temporal and sequential data. The focus then expanded to Transformer models and attention mechanisms, emphasizing their role in parallel processing, contextual representation, and generative modeling. Additional sections address fine-tuning and transfer learning, reinforcement learning for generative optimization, and latent variable models such as Variational Autoencoders for unsupervised learning and probabilistic generation.

This repository demonstrates a solid understanding of deep learning and generative modeling, combining theoretical insight with applied experimentation relevant to data science, artificial intelligence, and computational biology.

Tools and Libraries: Python, PyTorch, JAX, NumPy, Matplotlib, and Scikit-learn.
